{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import re\n",
    "import time\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract table schema from the database\n",
    "class TrainingPlanItem:\n",
    "    ITEM_TYPE_IS = \"Information_Schema\"\n",
    "    \n",
    "    def __init__(self, item_type, item_group, item_name, item_value):\n",
    "        self.item_type = item_type\n",
    "        self.item_group = item_group  # will be dynamic(e.g. database name)\n",
    "        self.item_name = item_name\n",
    "        self.item_value = item_value\n",
    "\n",
    "class TrainingPlan:\n",
    "    def __init__(self, plan=[]):\n",
    "        self._plan = plan\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self._plan.append(item)\n",
    "\n",
    "    def display_plan(self):\n",
    "        for item in self._plan:\n",
    "            print(f\"Group: {item.item_group}\\nName: {item.item_name}\\nValue:\\n{item.item_value}\\n\")\n",
    "    \n",
    "    def get_plan_as_text(self):\n",
    "        \"\"\"\n",
    "        Return the training plan as a string, which can be passed to the LLM or other systems.\n",
    "        \"\"\"\n",
    "        plan_text = \"\"\n",
    "        for item in self._plan:\n",
    "            plan_text += f\"Group: {item.item_group}\\n\"\n",
    "            plan_text += f\"Name: {item.item_name}\\n\"\n",
    "            plan_text += f\"Value:\\n{item.item_value}\\n\\n\"\n",
    "        return plan_text\n",
    "\n",
    "# Function to extract schema information from any SQLite database\n",
    "def get_training_plan_generic(cursor, conn) -> TrainingPlan:\n",
    "    # Create a new training plan\n",
    "    plan = TrainingPlan([])\n",
    "\n",
    "    # Extract the database name from the connection(SQLite doesn't have a real DB name)\n",
    "    # In other systems, you would extract the actual DB name.\n",
    "    cursor.execute(\"PRAGMA database_list;\")\n",
    "    databases = cursor.fetchall()\n",
    "    \n",
    "    # Generalized extraction of database name for the item_group\n",
    "    database_name = databases[0][2].split(\"\\\\\")[-1] # Fetch the database name from the PRAGMA query (second column is the name)\n",
    "\n",
    "    # Fetch all table names from the SQLite database\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    # Iterate through each table\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "\n",
    "        # Fetch table schema (column names and data types)\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "\n",
    "        # Create a document that summarizes the table structure\n",
    "        doc = f\"The following columns are in the {table_name} table in the {database_name} database:\\n\\n\"\n",
    "        \n",
    "        # DataFrame to format the markdown output\n",
    "        column_data = []\n",
    "        for column in columns:\n",
    "            column_data.append([column[1], column[2], column[3], column[4]])  # Column name, data type, not null, default value\n",
    "        \n",
    "        df_columns = pd.DataFrame(column_data, columns=[\"Column Name\", \"Data Type\", \"Not Null\", \"Default Value\"])\n",
    "        doc += df_columns.to_markdown()\n",
    "\n",
    "        # Addintg this information as a new training plan item\n",
    "        plan.add_item(\n",
    "            TrainingPlanItem(\n",
    "                item_type=TrainingPlanItem.ITEM_TYPE_IS,\n",
    "                item_group=database_name,  # Dynamic database name\n",
    "                item_name=table_name,\n",
    "                item_value=doc\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(client, user_question, ddl_schema, chatHistory):\n",
    "    # Get the schema of the database    \n",
    "    # Define the initial system message (including SQL documentation and guidelines)\n",
    "    initial_prompt = 'You are a SQL expert. Please help to generate a SQL query to answer the question. Your response should only be based on the given context and follow the response guidelines and format instructions.'\n",
    "    response_guidelines = (\n",
    "        \"\\n===Response Guidelines \\n\"\n",
    "        \"1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n\"\n",
    "        \"3. If the provided context is insufficient, please explain why it can't be generated. \\n\"\n",
    "        \"4. Please use the most relevant table(s). \\n\"\n",
    "        \"5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n\"\n",
    "        \"6. Ensure that the output SQL is SQL-compliant and executable, and free of syntax errors. \\n\"\n",
    "    )\n",
    "\n",
    "    # Combine everything to create the system message for the model\n",
    "    system_message = initial_prompt + \"\\n\\n===Tables and their details:\\n\" + ddl_schema + response_guidelines\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Add previous messages from chatHistory\n",
    "    for msg in chatHistory:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_question})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    generated_response = response.choices[0].message.content\n",
    "    return generated_response\n",
    "\n",
    "def extract_sql(llm_response):\n",
    "        \"\"\"\n",
    "        Extracts the SQL query from the LLM response. This is useful in case the LLM response contains other information besides the SQL query.\n",
    "        Override this function if your LLM responses need custom extraction logic.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the llm_response contains a CTE (with clause), extract the last sql between WITH and ;\n",
    "        sqls = re.findall(r\"\\bWITH\\b .*?;\", llm_response, re.DOTALL)\n",
    "        if sqls:\n",
    "            sql = sqls[-1]\n",
    "            return sql.replace(\"\\n\", \" \")\n",
    "\n",
    "        # If the llm_response is not markdown formatted, extract last sql by finding select and ; in the response\n",
    "        sqls = re.findall(r\"SELECT.*?;\", llm_response, re.DOTALL)\n",
    "        if sqls:\n",
    "            sql = sqls[-1]\n",
    "            return sql.replace(\"\\n\", \" \")\n",
    "\n",
    "        # If the llm_response contains a markdown code block, with or without the sql tag, extract the last sql from it\n",
    "        sqls = re.findall(r\"```sql\\n(.*)```\", llm_response, re.DOTALL)\n",
    "        if sqls:\n",
    "            sql = sqls[-1]\n",
    "            return sql.replace(\"\\n\", \" \")\n",
    "\n",
    "        sqls = re.findall(r\"```(.*)```\", llm_response, re.DOTALL)\n",
    "        if sqls:\n",
    "            sql = sqls[-1]\n",
    "            return sql.replace(\"\\n\", \" \")\n",
    "\n",
    "def run_sql(query, connection):\n",
    "    # Use pandas to execute the SQL query and store the results in a DataFrame\n",
    "    df = pd.read_sql_query(query, connection)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_followup_questions(client, question, sql, df, n_questions = 3, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate a list of followup questions\n",
    "        \"\"\"\n",
    "        \n",
    "        system_message = f\"You are a helpful data assistant. The user asked the question: '{question}'\\n\\nThe SQL query for this question was: {sql}\\n\\nThe following is a pandas DataFrame with the results of the query: \\n{df.to_markdown()}\\n\\n\"\n",
    "        user_message = f\"Generate a list of {n_questions} followup questions that the user might ask about this data. Respond with a list of questions, one per line. Do not answer with any explanations -- just the questions. Remember that there should be an unambiguous SQL query that can be generated from the question. Prefer questions that are answerable outside of the context of this conversation. Prefer questions that are slight modifications of the SQL query that was generated that allow digging deeper into the data. Each question will be turned into a button that the user can click to generate a new SQL query so don't use 'example' type questions. Each question must have a one-to-one correspondence with an instantiated SQL query.\"\n",
    "        \n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        )\n",
    "\n",
    "        llm_response =response.choices[0].message.content\n",
    "\n",
    "        numbers_removed = re.sub(r\"^\\d+\\.\\s*\", \"\", llm_response, flags=re.MULTILINE)\n",
    "        return numbers_removed.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the database connection\n",
    "conn = sqlite3.connect(\"C://Users//VRBRAHMB//Documents//sqlite-tools//chinook.db\")\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_plan = get_training_plan_generic(cursor, conn=conn)\n",
    "ddl_schema = training_plan.get_plan_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azureopenai_cleint(key, version, endpoint, deployment):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=key,  \n",
    "        api_version=version,\n",
    "        azure_endpoint = endpoint,\n",
    "        azure_deployment=deployment\n",
    "        )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatManager:\n",
    "    def __init__(self, base_directory: str = \"./chats\"):\n",
    "        self.base_directory = base_directory\n",
    "        self.embed_model = HuggingFaceEmbeddings(\n",
    "            model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "        )\n",
    "        self.active_chat_id = None\n",
    "        self.active_vectorstore = None\n",
    "        self.chat_history: Dict[str, List] = {}\n",
    "        \n",
    "        # Create base directory if it doesn't exist\n",
    "        if not os.path.exists(base_directory):\n",
    "            os.makedirs(base_directory)\n",
    "            \n",
    "        # Load existing chat IDs\n",
    "        self.existing_chats = self._load_existing_chats()\n",
    "        \n",
    "    def _load_existing_chats(self) -> List[str]:\n",
    "        \"\"\"Load all existing chat IDs from the base directory.\"\"\"\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            return []\n",
    "        return [d for d in os.listdir(self.base_directory) \n",
    "                if os.path.isdir(os.path.join(self.base_directory, d))]\n",
    "    \n",
    "    def _get_chat_directory(self, chat_id: str) -> str:\n",
    "        \"\"\"Get the directory path for a specific chat.\"\"\"\n",
    "        return os.path.join(self.base_directory, chat_id)\n",
    "\n",
    "    def _normalize_question(self, question: str) -> str:\n",
    "        \"\"\"Normalize question text to improve matching.\"\"\"\n",
    "        return ' '.join(question.lower().split())\n",
    "    \n",
    "    def initialize_chat(self, chat_id: str) -> None:\n",
    "        \"\"\"Initialize or switch to a specific chat.\"\"\"\n",
    "        chat_dir = self._get_chat_directory(chat_id)\n",
    "        \n",
    "        # If this is a new chat\n",
    "        if chat_id not in self.existing_chats:\n",
    "            os.makedirs(chat_dir, exist_ok=True)\n",
    "            self.chat_history[chat_id] = []\n",
    "            self.existing_chats.append(chat_id)\n",
    "            \n",
    "        # Initialize or switch to the chat's vector store\n",
    "        self.active_vectorstore = Chroma(\n",
    "            embedding_function=self.embed_model,\n",
    "            persist_directory=os.path.join(chat_dir, \"chroma_db\")\n",
    "        )\n",
    "        self.active_chat_id = chat_id\n",
    "        \n",
    "        # Load chat history if it exists\n",
    "        history_path = os.path.join(chat_dir, \"chat_history.json\")\n",
    "        if os.path.exists(history_path):\n",
    "            with open(history_path, 'r') as f:\n",
    "                self.chat_history[chat_id] = json.load(f)\n",
    "\n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        return np.dot(vec1, vec2) / (norm1 * norm2) if norm1 and norm2 else 0\n",
    "                \n",
    "    def save_chat_history(self) -> None:\n",
    "        \"\"\"Save the current chat history to disk.\"\"\"\n",
    "        if self.active_chat_id:\n",
    "            history_path = os.path.join(\n",
    "                self._get_chat_directory(self.active_chat_id),\n",
    "                \"chat_history.json\"\n",
    "            )\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(self.chat_history[self.active_chat_id], f)\n",
    "\n",
    "    def find_similar_question(self, question: str, question_embedding: List[float], \n",
    "                            similarity_threshold: float) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Find a similar question in the vector store.\n",
    "        Returns (found, metadata) tuple.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get all stored questions\n",
    "            collection = self.active_vectorstore.get()\n",
    "            \n",
    "            if not collection['ids']:  # Check if the collection is empty\n",
    "                return False, None\n",
    "\n",
    "            # Perform similarity search using ChromaDB's search functionality\n",
    "            results = self.active_vectorstore.similarity_search_with_relevance_scores(\n",
    "                question,\n",
    "                k=1  # Get the most similar result\n",
    "            )\n",
    "\n",
    "            if not results:\n",
    "                return False, None\n",
    "\n",
    "            most_similar_doc, similarity_score = results[0]\n",
    "            \n",
    "            if similarity_score >= similarity_threshold:\n",
    "                metadata = most_similar_doc.metadata\n",
    "                return True, metadata\n",
    "            else:\n",
    "                return False, None\n",
    "\n",
    "        except Exception as e:\n",
    "            return False, None\n",
    "                \n",
    "    def ask(self, client, question: str, db_connection, ddl_schema, \n",
    "            similarity_threshold: float = 0.85) -> Tuple[str, pd.DataFrame]:\n",
    "        \"\"\"Process a question in the context of the current chat.\"\"\"\n",
    "        if not self.active_chat_id:\n",
    "            raise ValueError(\"No active chat session. Please initialize a chat first.\")\n",
    "                \n",
    "        # Normalize the question\n",
    "        normalized_question = self._normalize_question(question)\n",
    "        \n",
    "        # Generate embedding for the new question\n",
    "        question_embedding = self.embed_model.embed_query(normalized_question)\n",
    "        \n",
    "        # Check cache for similar questions\n",
    "        found_similar, metadata = self.find_similar_question(\n",
    "            normalized_question, \n",
    "            question_embedding, \n",
    "            similarity_threshold\n",
    "        )\n",
    "        \n",
    "        if found_similar and metadata:\n",
    "            sql_query = metadata['text2sql']\n",
    "            results = pd.DataFrame(json.loads(metadata['df']))\n",
    "        else:\n",
    "            chat_history = self.chat_history[self.active_chat_id]\n",
    "            chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "            \n",
    "            response = llm_response(client, question, ddl_schema, chat_history)\n",
    "            sql_query = extract_sql(response)\n",
    "            results = run_sql(sql_query, db_connection)\n",
    "            \n",
    "            # Cache the new question and results\n",
    "            results_json = results.to_json(orient='records')\n",
    "            \n",
    "            # Add to vector store with metadata\n",
    "            self.active_vectorstore.add_texts(\n",
    "                texts=[normalized_question],\n",
    "                metadatas=[{\n",
    "                    'text2sql': sql_query, \n",
    "                    'df': results_json,\n",
    "                    'original_question': question\n",
    "                }],\n",
    "                ids=[f\"{self.active_chat_id}_{str(time.time())}\"]\n",
    "            )\n",
    "            self.active_vectorstore.persist()\n",
    "            \n",
    "            # Update chat history\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": sql_query})\n",
    "            self.save_chat_history()\n",
    "            \n",
    "        return sql_query, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    chat_manager = ChatManager()\n",
    "    \n",
    "    key = \"your_api_key\"\n",
    "    version = \"version_name\"\n",
    "    endpoint = \"your_endpoint\"\n",
    "    deployment = \"deployment_name\"\n",
    "    client = get_azureopenai_cleint(key, version, endpoint, deployment)\n",
    "    \n",
    "    while True:\n",
    "        # Get chat ID from user\n",
    "        chat_id = input(\"Enter chat ID (or 'exit' to quit): \").strip()\n",
    "        \n",
    "        if chat_id.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Initialize chat session\n",
    "        chat_manager.initialize_chat(chat_id)\n",
    "        print(f\"Active chat session: {chat_id}\")\n",
    "        \n",
    "        while True:\n",
    "            user_question = input(f\"[Chat {chat_id}] Ask your question (or 'switch' to change chat): \")\n",
    "            \n",
    "            if user_question.lower() == 'switch':\n",
    "                break\n",
    "                \n",
    "            if user_question.lower() in ['exit', 'quit']:\n",
    "                return\n",
    "                \n",
    "            start_time = time.time()\n",
    "            sql_query, results = chat_manager.ask(client,\n",
    "                user_question, \n",
    "                conn, \n",
    "                ddl_schema\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(f\"\\nGenerated SQL Query: {sql_query}\")\n",
    "            print(f\"Query Results: {results}\\n\")\n",
    "            print(f'Time taken: {end_time - start_time:.2f} seconds')\n",
    "            \n",
    "            # Generate follow-up questions for the current context\n",
    "            followup_questions = generate_followup_questions(\n",
    "                client = client,\n",
    "                question=user_question,\n",
    "                sql=sql_query,\n",
    "                df=results,\n",
    "                n_questions=3\n",
    "            )\n",
    "            print(\"\\nSuggested follow-up questions:\")\n",
    "            for i, q in enumerate(followup_questions, 1):\n",
    "                print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genAIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
